{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all possibly relevant packages\n",
    "import IPython\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics.pairwise as pair\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, SparsePCA, KernelPCA\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return the auc values for each class\n",
    "def aucfun(clf, X_test, Y_test):    \n",
    "    y_bin = label_binarize(Y_test, classes=[1, 2, 3, 4])\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    Y_predict = clf.predict_proba(X_test)    \n",
    "    #Calculate AUC\n",
    "    for i in range(4):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], Y_predict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import training data and labels into dataframes\n",
    "X = pd.read_csv('trainingData.txt', sep='\\t', header = None)\n",
    "Y = pd.read_csv('trainingTruth.txt', sep='\\t', header = None)\n",
    "Y = np.array(Y).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first thing we did was realize that the baseline script was training and testing both on 100% of the training data set; we decided to start splitting the data set into training and testing subsets so that we could get a better sense for how the classifiers would ultimately perform on test data. Here we first try to run the data using the base RandomForest classifier again on our split dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.569332566168\n",
      "{0: 0.86014548202186658, 1: 0.71242058510233586, 2: 0.85638518053498403, 3: 0.59322531535450196}\n"
     ]
    }
   ],
   "source": [
    "#fill NaN's with zeros\n",
    "X_zeros = X.fillna(0)\n",
    "#split dataset into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_zeros, Y, test_size=0.20, random_state=25)\n",
    "\n",
    "#We commented this code out because OneVsRestClassifier is causing a bizarre error to appear.  This is the baseline code. \n",
    "#clf = OneVsRestClassifier(RandomForestClassifier(n_estimators = 2, random_state=25))\n",
    "#clf.fit(X_train, Y_train)\n",
    "#print(clf.score(X_test, Y_test))\n",
    "#print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then played with the parameter settings manually to see how much we could gain in accuracy without yet directly manipulating the data. Just doing that we saw below a gain of about 10% in prediction accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661392405063\n",
      "{0: 0.94674890063842565, 1: 0.81848013200943459, 2: 0.93385955447438374, 3: 0.70579568756602251}\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=50, random_state=25, criterion='entropy',\n",
    "                                                max_features='auto', min_impurity_split = 1e-6))\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then came up with two ideas on how to better deal with the missing values in the data. First, we thought about calculating the correlation coefficient between every pair of features for all examples within a certain class;\n",
    "the idea being that when we have NaN's, we can look at the feature that most strongly correlates with the feature whose values we're missing, build a regression model based on that coefficient and predict our missing values using that model.  Below, we calculate a correlation matrix for all features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dataframe with class labels included\n",
    "X_withclasses = X.copy()\n",
    "X_withclasses['class'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''This function gives us a list of correlation matrices for all feature pairs for each class'''\n",
    "def feat_corr():\n",
    "    corrs = []\n",
    "    for i in range(4):\n",
    "        classi = X_withclasses.loc[X_withclasses['class'] == i + 1]        \n",
    "        classi = classi.drop('class', 1)        \n",
    "        corrs.append(classi.corr(method = 'pearson'))        \n",
    "    return corrs\n",
    "\n",
    "#get correlation matrix for each pair of features for each class\n",
    "corrs = feat_corr()\n",
    "\n",
    "#fill diagonal values with 0 since all of them are 1 in a correlation matrix\n",
    "for i in corrs:\n",
    "    np.fill_diagonal(i.values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24997305653985449, 0.13750980947766309, 0.1903504295059652, 0.14457949316609414]\n"
     ]
    }
   ],
   "source": [
    "'''Finds the highest absolute value correlation coefficient between all pairs of features for a certain class'''  \n",
    "def max_corr(corrs):\n",
    "    return [np.absolute(i).max().max() for i in corrs]\n",
    "maxes = max_corr(corrs)    \n",
    "print(maxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above, we show the highest correlation values for each class.  We can see that even the highest correlation coefficient (we look at negative coefficients too in the form of absolute values) between any pair of features within a class is extremely low, indicating that none of the features has any significant linear relationship with any other feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAECCAYAAAAW+Nd4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBuLN33Ecp4H8AbB0HAZZlb4NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e4a0ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#find highest correlation coefficient to graph features\n",
    "max_feature = corrs[0].as_matrix()\n",
    "arg = max_feature.argmax()\n",
    "row, col = (arg//max_feature.shape[1], arg%max_feature.shape[1])\n",
    "\n",
    "#get features correlation matrix for class 1\n",
    "class1 = X_withclasses.loc[X_withclasses['class'] == 1]\n",
    "class1 = class1.drop('class', 1)\n",
    "corr1 = np.array(class1.corr(method = 'pearson'))\n",
    "\n",
    "#plot feature vs feature to get any idea of what is going on \n",
    "plt.scatter(class1[row], class1[col])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above we see that even the two features with the highest correlation coefficient in all 4 classes do not share a strong enough linear, or seemingly other, relationship to be used effectively for prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having discovered that we could not fill NaNs using the feature correlation idea, we came up with a second idea to address NaNs: We decided to fill NaNs by finding the row at the shortest Euclidean distance, and then using that row's corresponding feature values to populate the NaNs of the row with missing values.   \n",
    "We realized that comparing row pairwise distances was problematic because each pair could be missing a different number of dimensions. To remedy the problem, we decided to fill in the NaNs with zeros before computing distance, reasoning that given the distribution of the number of missing values within the examples (maximum 7 missing values), doing this was unlikely to have any significant impact on what the closest neighbors would turn out to be.\n",
    "One subtlety of our implementation is that in the case in which the closest vector is missing the same values as the vector whose NaN values we're trying to fill, we sort the distance matrix (only in this case since it is an expensive operation) and look at the second, third, etc...closest vectors, until we find the closest one that also contains numerical values for the corresponding NaN values in the original vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper function to replace_nans_nn\n",
    "def replace_nans_nn_helper(X):\n",
    "    #Create a matrix of pairwise distances between all of the points\n",
    "    dist = pair.pairwise_distances(X.fillna(value=0))\n",
    "    np.fill_diagonal(dist, np.NaN)\n",
    "    #Create an array of nearest neighbors for each row\n",
    "    closest = np.nanargmin(dist, axis = 1)\n",
    "    #Loop through the data and replace NaN's with nearest neighbors\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[0]):\n",
    "            if np.isnan(X[i][j]):\n",
    "                if not np.isnan(X[i][closest[j]]):\n",
    "                    X[i][j] = X[i][closest[j]]\n",
    "                else:\n",
    "                    X[i][j] = X[i][np.argsort(dist[j])[1]]\n",
    "    return X\n",
    "\n",
    "#Function to replace the NaN values with the corresponding feature from the nearest neighbor datapoint\n",
    "def replace_nans_nn(X,Y):\n",
    "    #Separate the data by class\n",
    "    Xdivided = {1: X.iloc[Y==1].reset_index(drop=True), 2: X.iloc[Y==2].reset_index(drop=True), \\\n",
    "            3: X.iloc[Y==3].reset_index(drop=True), 4: X.iloc[Y==4].reset_index(drop=True)}\n",
    "    #Replace the NaNs in each separated dataframe with nearest neighbor feature values and concatenate the dataframes\n",
    "    Xnew = pd.DataFrame()\n",
    "    for i in Xdivided:\n",
    "        Xdivided[i] = replace_nans_nn_helper(Xdivided[i])\n",
    "        Xnew = pd.concat([Xnew,Xdivided[i]], ignore_index=True)\n",
    "    #Add the label sorted column\n",
    "    Y = np.sort(Y)\n",
    "    return Xnew, Y\n",
    "\n",
    "# Create the new datasets with NaNs populated according to the Euclidean distance method\n",
    "X_dist, Y_sorted = replace_nans_nn(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665995397008\n",
      "{0: 0.94222793584212039, 1: 0.82105054185351267, 2: 0.93616419626364955, 3: 0.69799266716935926}\n"
     ]
    }
   ],
   "source": [
    "# Check the classification performance using the new dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_dist, Y_sorted, test_size=0.20, random_state=25)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The classification results are roughly the same as they were when we simply replaced the NaNs with zeros. We decided to try a third method for replacing NaNs:  We replaced them with the feature mean for rows within the same class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide the data into classes and populate the NaNs using the within class feature means. \n",
    "classes = [X.iloc[np.array(Y == i+1)] for i in range(4)]\n",
    "classes = [i.apply(lambda x: x.fillna(x.mean()),axis=0) for i in classes]\n",
    "X_mean = pd.concat(classes)\n",
    "Y_sorted = np.sort(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672324510932\n",
      "{0: 0.94777607184931689, 1: 0.82420870702541116, 2: 0.93647402512516575, 3: 0.70287158744880029}\n"
     ]
    }
   ],
   "source": [
    "# Check the classification performance using the new dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_mean, Y_sorted, test_size=0.20, random_state=25)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see here that the class mean method for filling NaNs seems to provide superior accuracy to our previous more complex and time consuming correlation and Euclidean distance methods for replacing NaNs. We then decided to look at how many rows were missing values and how many they were missing, and came up with one more way to deal with NaNs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5759, 1: 6355, 2: 3517, 3: 1319, 4: 348, 5: 70, 6: 9, 7: 1})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of rows for each possible number of missing features within dataset\n",
    "nulls = X.isnull().sum(axis=1).tolist()\n",
    "Counter(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking above at the results of the count of rows for each possible number of missing values within our dataset,\n",
    "we came up with one last strategy to deal with NaNs, which was to remove the rows with 4 or more missing values, which effectively would remove only 2-3% of the rows in our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataset without rows that contain more than 3 missing values\n",
    "indices = np.array(X.isnull().sum(axis=1) < 4)\n",
    "X_nan_limit = X.iloc[indices].reset_index(drop=True)\n",
    "Y_nan_limit = Y[indices]\n",
    "\n",
    "# Divide the data into classes and populate the NaNs using the within class feature means. \n",
    "classes = [X_nan_limit.iloc[np.array(Y_nan_limit == i+1)] for i in range(4)]\n",
    "classes = [i.apply(lambda x: x.fillna(x.mean()),axis=0) for i in classes]\n",
    "X_nan_limit = pd.concat(classes)\n",
    "Y_nan_limit_sorted = np.sort(Y_nan_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671386430678\n",
      "{0: 0.94896770767778493, 1: 0.81372207920420592, 2: 0.93774760216847386, 3: 0.70967207556240741}\n"
     ]
    }
   ],
   "source": [
    "# Check the classification performance using the new dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_nan_limit, Y_nan_limit_sorted, test_size=0.20, random_state=25)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Despite small differences between all of those methods, our mean_per_class method is still the highest performing of all and therfore the one we ended up using throughout the rest of our exploration process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, we wanted to start exploring other classifiers and other classification methods, so we decided to standardize our dataset, which while not making any difference for algorithms such as RandomForests, is important for classifiers such as Neural Networks and Support Vector Machines and will facilitate dimensionality reduction techniques such as PCA and LDA later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the data to have mean zero and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(X_mean)\n",
    "X = pd.DataFrame(scaler.transform(X_mean))\n",
    "Y = Y_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We next wanted to test whether balancing classes to have the same number of rows per class would have a positive effect on performance. We took the class with the least number of rows and randomly extracted an equal number of rows from every other class before turning all of those subsets into a new balanced dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the data into classes and populate the NaNs using the within class feature means. \n",
    "classes = [X.iloc[np.array(Y == i+1)] for i in range(4)]\n",
    "classes = [i.apply(lambda x: x.fillna(x.mean()),axis=0) for i in classes]\n",
    "X_mean = pd.concat(classes)\n",
    "Y_sorted = np.sort(Y)\n",
    "\n",
    "X_balanced = X.copy()\n",
    "X_balanced['class'] = Y\n",
    "\n",
    "class1 = X_balanced.loc[X_balanced['class'] == 1]\n",
    "class2 = X_balanced.loc[X_balanced['class'] == 2]\n",
    "class3 = X_balanced.loc[X_balanced['class'] == 3]\n",
    "class4 = X_balanced.loc[X_balanced['class'] == 4]\n",
    "\n",
    "classes = [class1, class2, class3, class4]\n",
    "\n",
    "#balance classes \n",
    "balclasses = [i.sample(classes[3].shape[0]) for i in classes]\n",
    "\n",
    "X_balanced = pd.concat(balclasses)\n",
    "Y_balanced = X_balanced['class']\n",
    "X_balanced = X_balanced.drop('class', 1)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_balanced, Y_balanced, test_size=0.20, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642960115329\n",
      "{0: 0.93526649029813613, 1: 0.77101316911757134, 2: 0.9137730789913957, 3: 0.70848548519538224}\n"
     ]
    }
   ],
   "source": [
    "#test balanced dataset with RF\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_estimators=50, random_state=25, criterion='entropy',\n",
    "                                                max_features='auto', min_impurity_split = 1e-6))\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the drop in performance is significant (~4%) compared to using the full dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our hand-tuned multi-layer Perceptron Neural Network turned out to be the single highest performing classifier on the entire data set. Given how time consuming running the more complex models turned out to be, we initially in the first part of our exploration resorted mostly to hand-tuning the classifiers through trial and error, and common sense.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770713463751\n",
      "{0: 0.9710276833462067, 1: 0.90908211883408074, 2: 0.96422296145839548, 3: 0.85577693586991666}\n"
     ]
    }
   ],
   "source": [
    "def MLPclf():\n",
    "    return MLPClassifier(solver='sgd', alpha=10, hidden_layer_sizes=(50, 50, 50), random_state=1, max_iter=1000, \n",
    "                    activation = 'identity', learning_rate_init=0.001, momentum=0.9)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=25)\n",
    "\n",
    "clf = MLPclf()\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, before delving into ensemble methods we felt it would be important to look into dimensionality reduction techniques to reduce the feature space, allowing us from a practical perspective to train models faster and therefore tune parameters more efficiently, while also potentially improving performance. Below we create datasets based on different dimensionality reduction techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we shuffle our full sized dataset\n",
    "shuffled = np.random.permutation(X.index)\n",
    "X = X.reindex(shuffled)\n",
    "Y = Y[shuffled]\n",
    "\n",
    "# Function to look for combination of most important features from each class\n",
    "def get_imp_features(X,Y):\n",
    "    clf = OneVsRestClassifier(RandomForestClassifier(n_estimators = 30, criterion='entropy', \n",
    "                                                     max_features='auto'))\n",
    "    clf.fit(X,Y)\n",
    "    return np.unique([[x[1] for x in sorted(zip(i.feature_importances_, X.columns), reverse=True)[0:5]] \n",
    "              for i in clf.estimators_])\n",
    "\n",
    "# Create a reduced dimensionality dataset with SVD\n",
    "svd = TruncatedSVD(n_components=30, algorithm='randomized', n_iter=10)\n",
    "svd.fit(X)\n",
    "X_svd = pd.DataFrame(svd.transform(X))\n",
    "\n",
    "# Create a dataset containing only important features\n",
    "imp_features = get_imp_features(X, Y)\n",
    "X_imp_features = X[imp_features]\n",
    "\n",
    "# Create a dataset using Linear Discriminant Analysis transformation\n",
    "clf = LDA()\n",
    "clf.fit(X, Y)\n",
    "X_lda = clf.transform(X)\n",
    "\n",
    "# Create a dataset using Principal Component Analysis transformation\n",
    "clf = PCA(n_components='mle', svd_solver='full', iterated_power=10)\n",
    "clf.fit(X, Y)\n",
    "X_pca = clf.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we use cross validation to compare the how the original dataset (with all 334 feature dimensions) compares to each of the reduced dimension datasets (the important features dataset, the SVD dataset, the PCA dataset, and the LDA dataset). We test each dataset using a variety of classifiers to find out which data transformation is optimal for each classifier; however, given how computationally expensive and time consuming this would be, we decided to do the exhaustive testing using only 10% of each dataset. This process reveals the optimal dataset-classifier combination.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:694: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:694: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:694: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "clf1 = SVC()\n",
    "clf2 = GradientBoostingClassifier()\n",
    "clf3 = LogisticRegression(solver='lbfgs', multi_class = 'multinomial', max_iter = 300)\n",
    "clf4 = GaussianNB()\n",
    "clf5 = OneVsRestClassifier(RandomForestClassifier(n_estimators=10, random_state=None, criterion='entropy',\n",
    "                                                max_features='auto', min_impurity_split = 1e-6))\n",
    "clf6 = MLPclf()\n",
    "clf7 = LDA()\n",
    "clf8 = QDA()\n",
    "clf9 = KNeighborsClassifier(n_neighbors=180)\n",
    "\n",
    "clfs = [clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8, clf9]\n",
    "\n",
    "\n",
    "#original dataset with mean per class NaN filled values\n",
    "X_train, _ , Y_train , _ = train_test_split(X, Y, test_size=0.9, random_state=25)\n",
    "full_data = [np.mean(cross_val_score(classifier, X_train, Y_train, cv=3)) for classifier in clfs]\n",
    "\n",
    "X_train, _ , Y_train , _ = train_test_split(X_svd, Y, test_size=0.9, random_state=25)\n",
    "svd_data = [np.mean(cross_val_score(classifier, X_train, Y_train, cv=3)) for classifier in clfs]\n",
    "\n",
    "X_train, _ , Y_train , _ = train_test_split(X_imp_features, Y, test_size=0.9, random_state=25)\n",
    "imp_feat_data = [np.mean(cross_val_score(classifier, X_train, Y_train, cv=3)) for classifier in clfs]\n",
    "\n",
    "X_train, _ , Y_train , _ = train_test_split(X_lda, Y, test_size=0.9, random_state=25)\n",
    "lda_data = [np.mean(cross_val_score(classifier, X_train, Y_train, cv=3)) for classifier in clfs]\n",
    "\n",
    "X_train, _ , Y_train , _ = train_test_split(X_pca, Y, test_size=0.9, random_state=25)\n",
    "pca_data = [np.mean(cross_val_score(classifier, X_train, Y_train, cv=3)) for classifier in clfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The warnings we get are from running LDA and QDA classifiers on certain datasets and essentially indicate that those datasets are undesirable to use for those classifiers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we look at a few different combinations of measures between datasets and classifiers: First we look at how each dataset performed on average across all classifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.628923711236 0.752442837562 0.787367591096 0.748668322625 0.542130698072\n"
     ]
    }
   ],
   "source": [
    "#print means of lists to find best performing dataset overall\n",
    "print(np.mean(full_data), np.mean(svd_data), np.mean(lda_data), np.mean(pca_data), np.mean(imp_feat_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that the LDA transformed dataset performs significantly better on average than any other dataset.\n",
    "We next look into more detail at a breakdown of every classifier's performance on each dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_data</th>\n",
       "      <th>imp_feat_data</th>\n",
       "      <th>lda_data</th>\n",
       "      <th>pca_data</th>\n",
       "      <th>svd_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'SVC'</th>\n",
       "      <td>0.744382</td>\n",
       "      <td>0.553263</td>\n",
       "      <td>0.796198</td>\n",
       "      <td>0.787573</td>\n",
       "      <td>0.788711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'GBC'</th>\n",
       "      <td>0.648821</td>\n",
       "      <td>0.536571</td>\n",
       "      <td>0.766265</td>\n",
       "      <td>0.742065</td>\n",
       "      <td>0.751869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'LR'</th>\n",
       "      <td>0.656295</td>\n",
       "      <td>0.560172</td>\n",
       "      <td>0.793883</td>\n",
       "      <td>0.760513</td>\n",
       "      <td>0.760512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'GNB'</th>\n",
       "      <td>0.750132</td>\n",
       "      <td>0.560751</td>\n",
       "      <td>0.796192</td>\n",
       "      <td>0.774308</td>\n",
       "      <td>0.766852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'RF'</th>\n",
       "      <td>0.528494</td>\n",
       "      <td>0.505481</td>\n",
       "      <td>0.765114</td>\n",
       "      <td>0.730563</td>\n",
       "      <td>0.735738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'MLP'</th>\n",
       "      <td>0.696607</td>\n",
       "      <td>0.545192</td>\n",
       "      <td>0.781228</td>\n",
       "      <td>0.757619</td>\n",
       "      <td>0.751861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'LDA'</th>\n",
       "      <td>0.702360</td>\n",
       "      <td>0.565929</td>\n",
       "      <td>0.797917</td>\n",
       "      <td>0.765108</td>\n",
       "      <td>0.770286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'QDA'</th>\n",
       "      <td>0.334485</td>\n",
       "      <td>0.529651</td>\n",
       "      <td>0.797351</td>\n",
       "      <td>0.753601</td>\n",
       "      <td>0.769140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'KNN'</th>\n",
       "      <td>0.598737</td>\n",
       "      <td>0.522166</td>\n",
       "      <td>0.792160</td>\n",
       "      <td>0.666665</td>\n",
       "      <td>0.677018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_data  imp_feat_data  lda_data  pca_data  svd_data\n",
       "'SVC'   0.744382       0.553263  0.796198  0.787573  0.788711\n",
       "'GBC'   0.648821       0.536571  0.766265  0.742065  0.751869\n",
       "'LR'    0.656295       0.560172  0.793883  0.760513  0.760512\n",
       "'GNB'   0.750132       0.560751  0.796192  0.774308  0.766852\n",
       "'RF'    0.528494       0.505481  0.765114  0.730563  0.735738\n",
       "'MLP'   0.696607       0.545192  0.781228  0.757619  0.751861\n",
       "'LDA'   0.702360       0.565929  0.797917  0.765108  0.770286\n",
       "'QDA'   0.334485       0.529651  0.797351  0.753601  0.769140\n",
       "'KNN'   0.598737       0.522166  0.792160  0.666665  0.677018"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe of classifiers vs datasets\n",
    "sets_clfs = pd.DataFrame(\n",
    "    {'full_data': full_data,'svd_data': svd_data,'lda_data': lda_data, \n",
    "     'pca_data': pca_data, 'imp_feat_data': imp_feat_data}, \n",
    "    index=\"'SVC' 'GBC' 'LR' 'GNB' 'RF' 'MLP' 'LDA' 'QDA' 'KNN'\".split())\n",
    "\n",
    "sets_clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we can see that each classifier has its best performance on the LDA dataset, in most cases by a very significant margin; the most impressive change in prediction accuracy coming from the K Nearest Neighbors classifier, which performs poorly on every other dataset. Therefore we will from now on focus our exploration efforts on the LDA transformed dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now move on to tuning our classifiers using Grid Search. While we used 80% of our LDA transformed dataset to run Grid Serch in our code, given the expensiveness of those operations and the time it would take to run here, we will in this story iPython notebook only use 10% of the dataset in order to do Grid Search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_neighbors': 250}, 0.78123200921128377)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test , Y_train , Y_test = train_test_split(X_lda, Y, test_size=0.9, random_state=25)\n",
    "\n",
    "\n",
    "param_test1 = {'n_neighbors':[100, 150, 180, 200, 250, 300]}\n",
    "gsearch1 = GridSearchCV(estimator = KNeighborsClassifier(), \n",
    "                        param_grid = param_test1, scoring='accuracy', cv=10)\n",
    "gsearch1.fit(X_train, Y_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 0.9, 'gamma': 0.06, 'kernel': 'rbf'}, 0.78672133505970365)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train SVC on more of the data because it runs a lot faster\n",
    "X_train_SVC, X_test_SVC , Y_train_SVC , Y_test_SVC = train_test_split(X_lda, Y, test_size=0.2, random_state=25)\n",
    "\n",
    "param_test2 = {'C':[0.9, 1.0, 1.1], 'gamma':[0.04, 0.05, 0.06], \n",
    "               'kernel':['linear', 'rbf']}\n",
    "gsearch2 = GridSearchCV(estimator = SVC(), \n",
    "                        param_grid = param_test2, scoring='accuracy', cv=2)\n",
    "gsearch2.fit(X_train_SVC, Y_train_SVC)\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'criterion': 'entropy', 'max_features': 'auto', 'n_estimators': 80},\n",
       " 0.76511226252158893)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {'n_estimators':[50, 70, 80, 90, 100], 'max_features':['auto', 'log2', 1.0],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "gsearch3 = GridSearchCV(estimator = RandomForestClassifier(), \n",
    "                        param_grid = param_test3, scoring='accuracy', cv=2)\n",
    "gsearch3.fit(X_train, Y_train)\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Neural Network Grid Search takes several hours to run and we are therefore commenting it out here.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'activation': 'identity',\n",
       "  'alpha': 0.001,\n",
       "  'hidden_layer_sizes': (50, 50),\n",
       "  'learning_rate': 'adaptive',\n",
       "  'learning_rate_init': 0.01,\n",
       "  'max_iter': 10000,\n",
       "  'solver': 'adam'},\n",
       " 0.78612716763005785)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#param_test4 = {'hidden_layer_sizes':[(10, 10),(10, 10, 10),(50, 50),(50, 50, 50)], \n",
    "#               'activation':['identity', 'relu'],\n",
    "#               'solver':['adam', 'lbfgs', 'sgd'], 'alpha':[0.1, 0.01, 0.001],\n",
    "#               'learning_rate':['constant', 'invscaling', 'adaptive'], 'max_iter':[10000],\n",
    "#               'learning_rate_init':[0.1, 0.01, 0.001]}\n",
    "#gsearch4 = GridSearchCV(estimator = MLPClassifier(), \n",
    "#                        param_grid = param_test4, scoring='accuracy', cv=2)\n",
    "\n",
    "#gsearch4.fit(X_train2, Y_train2)\n",
    "#gsearch4.best_params_, gsearch4.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We store our tuned classifiers into functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(criterion='entropy', max_features='auto', n_estimators=80)\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=250)\n",
    "\n",
    "MLP = MLPClassifier(hidden_layer_sizes = (50, 50), activation='identity', solver='adam', alpha = 0.001\n",
    "                 , learning_rate = 'adaptive', max_iter=10000, learning_rate_init = 0.01)\n",
    "\n",
    "SVM = SVC(C=0.5, gamma=0.01, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we try our presumably improved classifiers on the LDA dataset again. While the results below show no improvement or even marginal decrease in performance, the results on training on most of the training dataset, instead of just 10% of it for the puporse of this story notebook, showed improvements in performance overall.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_lda_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'RF'</th>\n",
       "      <td>0.761086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'KNN'</th>\n",
       "      <td>0.774894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'MLP'</th>\n",
       "      <td>0.785830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'SVM'</th>\n",
       "      <td>0.784094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       new_lda_data\n",
       "'RF'       0.761086\n",
       "'KNN'      0.774894\n",
       "'MLP'      0.785830\n",
       "'SVM'      0.784094"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_clfs = [RF, KNN, MLP, SVM]\n",
    "new_X_train, _ , new_Y_train , _ = train_test_split(X_lda, Y, test_size=0.9, random_state=25)\n",
    "new_lda_data = [np.mean(cross_val_score(classifier, new_X_train, new_Y_train, cv=3)) for classifier in new_clfs]\n",
    "\n",
    "#create dataframe of classifiers vs datasets\n",
    "sets_new_clfs = pd.DataFrame(\n",
    "    {'new_lda_data': new_lda_data}, \n",
    "    index=\"'RF' 'KNN' 'MLP' 'SVM'\".split())\n",
    "\n",
    "sets_new_clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have tuned our classifiers, we move on to trying ensemble methods. We try to apply bagging to KNN, MLP and SVM.\n",
    "We first create a function that computes the sum of the AUC scores, a metric which will be very useful when we look at every possible combination of classifiers during our ensemble voting section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''This function returns the sum of the AUC scores for all 4 classes'''\n",
    "def aucsum(auc_scores):\n",
    "    auc_sum = 0.\n",
    "    for i in range(4):\n",
    "        auc_sum += auc_scores[i]\n",
    "    return auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with 250 neighbors: Accuracy = 0.781983249153 AUC scores = {0: 0.97259052268509005, 1: 0.91723363900964872, 2: 0.96505118859363048, 3: 0.86490348214543133} AUC Sum = 3.71977883243\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(base_estimator = KNN, n_estimators=10)\n",
    "clf.fit(X_train, Y_train)\n",
    "print('KNN with 250 neighbors: Accuracy =', clf.score(X_test, Y_test), 'AUC scores =', aucfun(clf, X_test, Y_test),\n",
    "     'AUC Sum =', aucsum(aucfun(clf, X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: Accuracy = 0.78792916054 AUC Scores = {0: 0.93210946800590055, 1: 0.81166242465218019, 2: 0.92626451330852599, 3: 0.74318048094252609} AUC Sum = 3.41321688691\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(base_estimator = SVM, n_estimators = 10)\n",
    "clf.fit(X_train, Y_train)\n",
    "print('SVC: Accuracy =', clf.score(X_test, Y_test), 'AUC Scores =', aucfun(clf, X_test, Y_test),\n",
    "     'AUC Sum =', aucsum(aucfun(clf, X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: Accuracy = 0.789143916629 AUC Scores = {0: 0.97442757488410658, 1: 0.91831665504921467, 2: 0.96524190672378929, 3: 0.87640524906715911} AUC Sum = 3.73439138572\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(base_estimator = MLP, n_estimators = 10)\n",
    "clf.fit(X_train, Y_train)\n",
    "print('MLP: Accuracy =', clf.score(X_test, Y_test), 'AUC Scores =', aucfun(clf, X_test, Y_test),\n",
    "     'AUC Sum =', aucsum(aucfun(clf, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results of the Bagging ensemble method on KNN and SVM were disappointing, with a dramatic drop in AUC scores for the SVM classifier. However, we saw some improvement on the MLP classifier.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784668499457\n",
      "{0: 0.95559541149445271, 1: 0.9005983026766966, 2: 0.95188021935566625, 3: 0.83557938163871892}\n",
      "3.64365331517\n",
      "0.78671440445\n",
      "{0: 0.97448320597084703, 1: 0.91726753038907527, 2: 0.96492665334015304, 3: 0.87711159395135052}\n",
      "3.73378898365\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))\n",
    "\n",
    "clf = SVC(C=0.5, gamma=0.01, kernel='linear', probability=True)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.751550412378\n",
      "{0: 0.97374706956654389, 1: 0.89784691655551918, 2: 0.96385138200013443, 3: 0.7935110856655716}\n",
      "3.62895645379\n",
      "0.779489802442\n",
      "{0: 0.97418188020809826, 1: 0.916401599214833, 2: 0.96482105401461227, 3: 0.8766682020035923}\n",
      "3.73207273544\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver='sgd', alpha=10, hidden_layer_sizes=(50, 50, 50), random_state=1, max_iter=1000, \n",
    "                    activation = 'identity', learning_rate_init=0.001, momentum=0.9)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))\n",
    "\n",
    "clf = MLP\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782366856339\n",
      "{0: 0.97293780819835751, 1: 0.91760287851561972, 2: 0.96504581184589433, 3: 0.86764650829001666}\n",
      "3.72323300685\n",
      "0.785755386484\n",
      "{0: 0.97327051702907696, 1: 0.91808690914080082, 2: 0.96518694658601945, 3: 0.86961713025044607}\n",
      "3.72616150301\n"
     ]
    }
   ],
   "source": [
    "clf = KNN\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=180)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(clf.score(X_test, Y_test))\n",
    "print(aucfun(clf, X_test, Y_test))\n",
    "print(aucsum(aucfun(clf, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above, we see that running our tuned classifiers versus their old selves show significant (around 10% total) improvements in AUC scores for both the MLP and SVC classifiers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**After seeing mixed results with bagging, we tried using an ensemble voting classifier to benefit from the potentially different types of class prediction in our individual classifiers. We trained every possible combination of classifiers in a voting ensemble in an effort to find the best combination of classifiers (here we chose our best classfiers (only 6 of them) but initially ran through combinations of all 9 classifiers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all possible combinations of chosen classifiers\n",
    "classifiers = [('SVC', clf1), ('GBC', clf2), ('LR', clf3), ('GNB', clf4), \n",
    "                                     ('LDA', clf5), ('QDA', clf6)]\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(2, 3):    #number of classifiers in each model\n",
    "    for clfs in itertools.combinations(classifiers, i):  #generating combinations\n",
    "        models.append(clfs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>WARNING: The code below takes several hours to run. As such we weren't able to run it in this \"story\" iPython notebook specifically, meaning it is possible that the code won't function as intended in this context. If you wish to run it anyway, just uncomment the cells below.<font color='red'>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aucs = []\n",
    "accus = []\n",
    "count = len(models)\n",
    "\n",
    "\n",
    "# Train and record results for every possible combination of 1, 2, 3, 4, 5 and 6 classifiers\n",
    "#for k in range(count):    \n",
    "#    eclf = VotingClassifier(estimators=models[k], voting='soft')\n",
    "#    eclf.fit(X_train, Y_train)\n",
    "#\n",
    "#    Y_predict = eclf.predict_proba(X_test)\n",
    "#\n",
    "    # Binarize the output\n",
    "#    y_bin = label_binarize(Y_test, classes=[1,2,3,4])\n",
    "#\n",
    "    # Calculate AUC\n",
    "#    fpr = dict()\n",
    "#    tpr = dict()\n",
    "#    roc_auc = dict()\n",
    "#    for i in range(4):\n",
    "#        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], Y_predict[:, i])\n",
    "#        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "#    aucs.append(aucsum(eclf, X_test, Y_test))\n",
    "#    accus.append(eclf.score(X_test, Y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the sum of every dictionary of AUC values for each result in aucs list\n",
    "\n",
    "#sums = []\n",
    "#for au in aucs:\n",
    "#    sm = 0\n",
    "#    for j in au:\n",
    "#        sm += au[j]\n",
    "#    sums.append(sm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After gathering the AUC scores for every combination of classifiers in an ensemble voting classifier, we look for the highest performing classifier combination, both in terms of the sum of AUC scores and accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look for the greatest sum of AUC scores \n",
    "#np.argmax(sums)\n",
    "\n",
    "# Look for the greatest accuracy score\n",
    "#np.argmax(accus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We ultimately decided to keep a combination of the model with highest accuracy and the model with greatest sum of AUC scores, as they turned out to be different and we wanted to maximize a combination of those two criteria. As such our final model, which gave us an AUC scores sum and accuracy score between those of the two individual models, was made up of GaussianNB, QDA, LDA and KNN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We then decided to try using a stacking classifier with our best performing combination of classifiers. The results turned out to be slightly inferior to those of the voting classifier, so we decided to submit the voting classifier's predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789079982098\n",
      "{0: 0.97247328631121777, 1: 0.91954065270623875, 2: 0.96601998551796631, 3: 0.87418949128198786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barbaramaclaurin/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:457: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test , Y_train , Y_test = train_test_split(X_lda, Y, test_size=0.9, random_state=25)\n",
    "\n",
    "mod1 = KNeighborsClassifier(n_neighbors = 180).fit(X_train,Y_train)\n",
    "mod1_pred_train = mod1.predict_proba(X_train)\n",
    "mod1_pred_test = mod1.predict_proba(X_test)\n",
    "\n",
    "mod2 = LDA().fit(X_train,Y_train)\n",
    "mod2_pred_train = mod2.predict_proba(X_train)\n",
    "mod2_pred_test = mod2.predict_proba(X_test)\n",
    "\n",
    "mod3 = QDA().fit(X_train,Y_train)\n",
    "mod3_pred_train = mod3.predict_proba(X_train)\n",
    "mod3_pred_test = mod3.predict_proba(X_test)\n",
    "\n",
    "mod4 = GaussianNB().fit(X_train,Y_train)\n",
    "mod4_pred_train = mod4.predict_proba(X_train)\n",
    "mod4_pred_test = mod4.predict_proba(X_test)\n",
    "\n",
    "\n",
    "#Creating training attributes for based on mod1, mod2, mod3, and mod4\n",
    "FeaturesTrain1 = np.hstack([mod1_pred_train,mod2_pred_train,mod3_pred_train,mod4_pred_train])  \n",
    "stacker = LogisticRegression(random_state=69).fit(FeaturesTrain1,Y_train)\n",
    "\n",
    "#Creating test attributes for Model3 (based on Model1 and Model2)\n",
    "FeaturesTest1 = np.hstack([mod1_pred_test,mod2_pred_test,mod3_pred_test,mod4_pred_test])\n",
    "\n",
    "#Final predictions\n",
    "score = stacker.score(FeaturesTest1, Y_test)\n",
    "print(score)\n",
    "print(aucfun(stacker, FeaturesTest1, Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>At that point, we had two important insights that allowed us to make great improvements in our performance.<font color='red'>**\n",
    "\n",
    "**We first realized that we needed to split the data prior to applying any tranformation in order to get accurate test results.**\n",
    "\n",
    "**We also realized that both PCA and LDA capture different types of information from the original dataset.  Both seemed important, so we decided to create a new dataset by combining the LDA transformed data with the first 12 columns (a number we came up with through an exhaustive iterative process) of the PCA transformed data.  To our delight, this new dataset achieved higher prediction accuracy than just the LDA transformed dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to Calculate the 4 AUC scores\n",
    "def aucfun(clf, X_test, Y_test):    \n",
    "    y_bin = label_binarize(Y_test, classes=[1, 2, 3, 4])\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    Y_predict = clf.predict_proba(X_test)    \n",
    "    #Calculate AUC\n",
    "    for i in range(4):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], Y_predict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    return roc_auc\n",
    "\n",
    "# Function to Sum the 4 AUC Scores\n",
    "def aucsum(clf, X_test, Y_test):\n",
    "    auc_scores = aucfun(clf, X_test, Y_test)\n",
    "    auc_sum = 0.\n",
    "    for i in range(4):\n",
    "        auc_sum += auc_scores[i]\n",
    "    return auc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import training data and labels into dataframes\n",
    "X = pd.read_csv('trainingData.txt', sep='\\t', header = None)\n",
    "Y = pd.read_csv('trainingTruth.txt', sep='\\t', header = None)\n",
    "Y = np.array(Y).ravel()\n",
    "\n",
    "#Fill NaNs with the within class mean feature values\n",
    "classes = [X.iloc[np.array(Y == i+1)] for i in range(4)]\n",
    "classes = [i.apply(lambda x: x.fillna(x.mean()),axis=0) for i in classes]\n",
    "X = pd.concat(classes)\n",
    "Y = np.concatenate((np.ones(len(classes[0])), 2*np.ones(len(classes[1])), \n",
    "                         3*np.ones(len(classes[2])), 4*np.ones(len(classes[3]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data for future testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the training data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_std = pd.DataFrame(scaler.transform(X_train))\n",
    "# Create a reduced feature dimensionality dataset using a PCA transformation\n",
    "pcaclf = PCA().fit(X_std)\n",
    "X_pca = pcaclf.transform(X_std)\n",
    "# Create a reduced feature dimensionality dataset using the LDA classifier transformation\n",
    "ldaclf = LDA().fit(X_std,Y_train)\n",
    "X_lda = ldaclf.transform(X_std)\n",
    "# Combine the first 12 columns of the PCA dataset with the 3 columns in the LDA dataset\n",
    "X_pca_lda = pd.concat([pd.DataFrame(X_pca[:,0:12]), pd.DataFrame(X_lda)], axis=1)\n",
    "\n",
    "# Transform the testing data using the standardization, LDA transformation, and PCA transformation created on the training data\n",
    "X_test_std = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_pca = pcaclf.transform(X_test_std)\n",
    "X_test_lda = ldaclf.transform(X_test_std)\n",
    "X_test_pca_lda = pd.concat([pd.DataFrame(X_test_pca[:,0:12]), pd.DataFrame(X_test_lda)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the same iterative process of testing every combination of voting ensemble base classifiers on our new mix of PCA and LDA dataset, we found our final optimal classification model, a voting ensemble classifier comprised of Quadratic Discriminant Analysis, Support Vector Machine and Multi-Layer Perceptron classifiers. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.98712878033750484, 1: 0.9355043512398169, 2: 0.97878602890163224, 3: 0.90782095691708165}\n",
      "3.8092401174\n",
      "0.819332566168\n"
     ]
    }
   ],
   "source": [
    "# Classification Script\n",
    "\n",
    "#Create the optimal classifier.  It is a voting classifier that is fed the following 3 sub classifiers:\n",
    "#    -Quadratic Discriminant Analysis Classifier\n",
    "#    -Support Vector Machine\n",
    "#    -Multilayer Perceptron Classifier\n",
    "clfs = [('qda', QDA()), ('svc', SVC(probability=True)), ('mlp', MLPClassifier())]\n",
    "clf = VotingClassifier(estimators = clfs, voting='soft')\n",
    "clf.fit(X_pca_lda, Y_train)\n",
    "Y_predict = clf.predict_proba(X_test_pca_lda)\n",
    "# Binarize the output\n",
    "y_bin = label_binarize(Y_test, classes=[1, 2, 3, 4])\n",
    "\n",
    "#Calculate AUC and scores using the reserved test data\n",
    "print(aucfun(clf, X_test_pca_lda,Y_test))\n",
    "print(aucsum(clf,X_test_pca_lda,Y_test))\n",
    "print(clf.score(X_test_pca_lda, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
